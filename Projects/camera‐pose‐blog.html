<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  <title>Camera-Pose + Attention: My MATH 156 Final Project</title>
  <style>
    body {
      font-family: Arial, sans-serif;
      line-height: 1.6;
      max-width: 800px;
      margin: 0 auto;
      padding: 20px;
      background-color: #f9f9f9;
      color: #333;
    }
    header, footer {
      text-align: center;
      margin-bottom: 40px;
    }
    h1 {
      font-size: 2.5em;
      margin-bottom: 10px;
    }
    h2 {
      font-size: 2em;
      margin-top: 40px;
      margin-bottom: 10px;
      border-bottom: 2px solid #ddd;
      padding-bottom: 5px;
    }
    h3 {
      font-size: 1.5em;
      margin-top: 30px;
      margin-bottom: 10px;
    }
    p {
      margin-bottom: 20px;
    }
    img {
      max-width: 100%;
      height: auto;
      margin: 20px 0;
      border: 1px solid #ccc;
      border-radius: 4px;
    }
    pre {
      background-color: #2d2d2d;
      color: #f8f8f2;
      padding: 15px;
      overflow-x: auto;
      border-radius: 4px;
      margin-bottom: 20px;
    }
    code {
      font-family: Consolas, Menlo, Monaco, "Courier New", monospace;
      font-size: 0.95em;
      color: #f8f8f2;
    }
    .caption {
      font-size: 0.9em;
      color: #555;
      text-align: center;
      margin-top: -15px;
      margin-bottom: 30px;
    }
    .table-container {
      overflow-x: auto;
      margin-bottom: 30px;
    }
    table {
      width: 100%;
      border-collapse: collapse;
      margin-bottom: 30px;
    }
    th, td {
      border: 1px solid #ddd;
      padding: 10px;
      text-align: left;
    }
    th {
      background-color: #f0f0f0;
    }
    a {
      color: #0066cc;
      text-decoration: none;
    }
    a:hover {
      text-decoration: underline;
    }
    .sidebar-link {
      margin-top: 40px;
      text-align: center;
    }
    .sidebar-link a {
      display: inline-block;
      padding: 8px 16px;
      background-color: #0066cc;
      color: #fff;
      border-radius: 4px;
    }
    .sidebar-link a:hover {
      background-color: #004999;
    }
  </style>
</head>
<body>

  <header>
    <h1>LightCBAM-ResNet: A Lightweight Attention-Enhanced Backbone
      for Camera Pose Estimation</h1>
    <p><em>By Yuer Tang (June 2025)</em></p>
  </header>

  <!-- Section 1: Opening Hook -->
  <section id="opening-hook">
    <h2>Opening Hook</h2>
    <p><strong>Ever tried to take a photo on a foggy evening, only to see your phone‚Äôs AR compass spin wildly? Or watched a drone fly down a canyon and lose its GPS signal? Camera‚Äêpose estimation‚Äîteaching a neural network to infer ‚ÄúWhere am I?‚Äù from a single image‚Äîsolves exactly that.</strong> In this post, we‚Äôll show how we took Google‚Äôs PoseNet concept and made it sharper, faster, and more robust by swapping in a ResNet backbone plus a lightweight attention module called CBAM. Along the way, you‚Äôll see how a little ‚Äúfocus‚Äù (both spatially and channel-wise) lets a network zoom in on the right pixels‚Äîand avoid getting fooled by passing crowds or shifting shadows.</p>
    <img src="images/hook-drone.jpg" alt="Drone navigating narrow canyon without GPS." />
    <p class="caption">Figure 1: A drone losing GPS in a narrow canyon.</p>
  </section>

  <!-- Section 2: Why Camera Pose Matters -->
  <section id="why-camera-pose">
    <h2>Why Camera Pose Matters</h2>
    <p><strong>Camera‚Äêpose estimation</strong> is the process of determining a camera‚Äôs 6‚ÄêDoF (degrees of freedom)‚Äîits position (x, y, z) and orientation (pitch, yaw, roll)‚Äîfrom a single RGB image. In many robotics and AR/VR applications, knowing exactly where the camera is and how it‚Äôs oriented is critical. For example:</p>
    <ul>
      <li><strong>Autonomous drones:</strong> In GPS‚Äêdenied environments (indoor warehouses, dense forests), a drone must rely on its camera to navigate safely around obstacles.</li>
      <li><strong>Augmented reality headsets:</strong> AR overlays must align precisely with the real world. A small pose error can break immersion or even cause motion sickness.</li>
      <li><strong>Self‚Äêdriving cars:</strong> Visual localization helps correct drift when LIDAR or GPS data is unreliable (e.g., urban canyons).</li>
    </ul>
    <p>The classic approach‚Äîfeature‚Äêtracking combined with a Kalman filter‚Äîworks well in textured, static environments. But in low‚Äêtexture scenes (plain walls, repetitive patterns) or dynamic settings (moving crowds, varying lighting), traditional pipelines struggle. That‚Äôs where deep‚Äêlearning‚Äìbased pose estimators like PoseNet come in: a CNN can learn to ‚Äúsee‚Äù the distinctive cues that humans might ignore.</p>
  </section>

  <!-- Section 3: From PoseNet to ResNet+CBAM -->
  <section id="from-posenet-to-resnet-cbam">
    <h2>From PoseNet to ResNet + CBAM: The Big Idea</h2>
    <h3>Brief Recap of PoseNet</h3>
    <p>In 2015, Kendall et al. proposed <strong>PoseNet</strong>‚Äîa convolutional neural network that takes a single image and directly regresses a 6‚ÄêDoF pose. PoseNet used a VGG16 backbone pre-trained on ImageNet and replaced the final classification layers with two fully connected heads‚Äîone for 3D position (x, y, z) and one for orientation (quaternion [w, x, y, z]). PoseNet was revolutionary, but it had two main limitations:</p>
    <ul>
      <li><strong>Sub‚Äêpar accuracy:</strong> VGG16 sometimes got distracted by uninformative pixels (e.g., sky, ground), leading to noisy pose estimates.</li>
      <li><strong>Overfitting:</strong> Because PoseNet relied on a fixed backbone, it tended to overfit to the training scene and didn‚Äôt generalize well to new views or lighting conditions.</li>
    </ul>

    <h3>What Is CBAM?</h3>
    <p>The <strong>Convolutional Block Attention Module (CBAM)</strong> is a lightweight plug-in introduced by Woo et al. in ECCV 2018. CBAM asks two simple questions at each layer:</p>
    <ol>
      <li><strong>Channel attention:</strong> ‚ÄúWhich feature maps (channels) are most informative for the task?‚Äù</li>
      <li><strong>Spatial attention:</strong> ‚ÄúWithin those maps, which spatial locations matter most?‚Äù</li>
    </ol>
    <p>CBAM implements channel attention via global average & max pooling ‚Üí a small MLP ‚Üí sigmoid gating. It implements spatial attention via pooling across the channel dimension ‚Üí a 7√ó7 convolution ‚Üí sigmoid gating. You can drop CBAM into any CNN without adding significant overhead.</p>
    <img src="Images/CBAM_model.jpg" /> alt="Diagram of CBAM channel and spatial attention modules." />
    <p class="caption">Figure 2: CBAM‚Äôs two‚Äêstage attention: channel‚Äêwise and spatial.</p>

    <h3>Why ResNet?</h3>
    <p><strong>ResNet</strong> (He et al., CVPR 2016) introduced skip‚Äêconnections (‚Äúidentity shortcuts‚Äù) to allow very deep networks to train without vanishing gradients. Compared to VGG16, ResNet (e.g., ResNet‚Äê50) has better feature extraction capacity, fewer parameters for a given depth, and more stable training. By combining ResNet50 with CBAM, we aimed to:</p>
    <ul>
      <li>Extract richer, deeper image features (thanks to ResNet‚Äê50‚Äôs skip connections).</li>
      <li>Guide the network‚Äôs attention to the most relevant pixels for localization (thanks to CBAM).</li>
    </ul>
    <p>In other words, ResNet50 provides the ‚Äúeyes,‚Äù and CBAM provides the ‚Äúfocus.‚Äù</p>
  </section>

  <!-- Section 4: High-Level Model Overview -->
  <section id="model-overview">
    <h2>High-Level Model Overview</h2>
    <p>Below is a simplified block diagram of our ResNet50 + CBAM pose estimator (full code available on GitHub <a href="https://github.com/YuerTang/Math-156-Project" target="_blank">here</a>):</p>
    <img src="images/model-block-diagram.png" alt="Block diagram of ResNet50+CBAM with two pose heads." />
    <p class="caption">Figure 3: ResNet50 backbone with CBAM after each stage, leading into two separate pose‚Äêregression heads.</p>

    <h3>Key Points</h3>
    <ul>
      <li><strong>Backbone:</strong> ResNet‚Äê50 pre-trained on ImageNet. We froze the first two residual stages for the first 10 epochs, then unfroze all layers for fine‚Äêtuning.</li>
      <li><strong>CBAM Placement:</strong> We inserted a CBAM block after each ResNet stage (conv2_x, conv3_x, conv4_x, conv5_x). Each CBAM receives the feature maps from its stage, applies channel & spatial attention, and feeds the refined features back into the network.</li>
      <li><strong>Pose Heads:</strong> After the last ResNet stage + CBAM, we apply global average pooling (GAP). The GAP output then goes into two parallel multi‚Äêlayer perceptrons (MLPs):</li>
        <ul>
          <li><strong>Translation head:</strong> Fully connected layers ‚Üí predicts (x, y, z).</li>
          <li><strong>Rotation head:</strong> Fully connected layers ‚Üí predicts (quaternion w, x, y, z).</li>
        </ul>
      <li><strong>Loss Functions:</strong> We experimented with two variants:  
        <ul>
          <li><strong>Static‚ÄêŒ≤ loss:</strong> A weighted sum of translation error (MSE) and rotation error (MSE) with a fixed weight Œ≤ = 0.01 (typical from Kendall et al. 2017).</li>
          <li><strong>Learnable‚ÄêŒ≤ loss:</strong> The network learns a per‚Äêtask uncertainty parameter œÉ<sub>t</sub> and œÉ<sub>r</sub>. The combined loss is:  
            <pre><code>ùêø = (1/œÉ<sub>t</sub><sup>2</sup>)‚ÄñÀÜx ‚àí x‚Äñ<sup>2</sup> + log œÉ<sub>t</sub><sup>2</sup>  
                 + (1/œÉ<sub>r</sub><sup>2</sup>)‚ÄñÀÜq ‚àí q‚Äñ<sup>2</sup> + log œÉ<sub>r</sub><sup>2</sup></code></pre>  
            This encourages the network to balance translation vs rotation loss dynamically.</li>
        </ul>
      </li>
    </ul>
  </section>

  <!-- Section 5: Our Dataset ‚Äì King‚Äôs College, Cambridge -->
  <section id="dataset">
    <h2>Our Dataset: King‚Äôs College, Cambridge</h2>
    <p>We used the <a href="http://mi.eng.cam.ac.uk/projects/relocalisation/#king's-college" target="_blank">King‚Äôs College dataset</a>, an outdoor benchmark captured by a handheld camera walking around the famous quad at King‚Äôs College, Cambridge. It consists of ~9,950 RGB frames, each labeled with a ground-truth 6-DoF pose (obtained via structure‚Äêfrom‚Äêmotion). Key challenges include:</p>
    <ul>
      <li><strong>Lighting variations:</strong> Shadows move as clouds pass, causing brightness to shift drastically between frames.</li>
      <li><strong>Dynamic elements:</strong> Pedestrians, bicycles, and cars occasionally enter the scene.</li>
      <li><strong>Repetitive architecture:</strong> Stone walls and archways that look very similar from different viewpoints, making it easy to confuse locations.</li>
    </ul>
    
    <p>We followed the standard train/test split provided on the dataset page: 70 % of the frames for training and 30 % for validation. No frames overlapped between the two splits, so we avoided any data leakage.</p>
  </section>

  <!-- Section 6: Training Procedure -->
  <section id="training-procedure">
    <h2>Training Procedure</h2>
    <p>Below is a summary of our key hyperparameters and training decisions for each model variant:</p>

    <div class="table-container">
      <table>
        <thead>
          <tr>
            <th>Model Variant</th>
            <th>Backbone</th>
            <th>CBAM?</th>
            <th>Loss Type</th>
            <th>Epochs</th>
            <th>Optimizer</th>
            <th>Learning Rate</th>
            <th>Batch Size</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td>VGG16-PoseNet</td>
            <td>VGG16 (ImageNet)</td>
            <td>No</td>
            <td>Static-Œ≤ (Œ≤=0.01)</td>
            <td>50</td>
            <td>Adam</td>
            <td>1√ó10<sup>‚àí4</sup></td>
            <td>32</td>
          </tr>
          <tr>
            <td>ResNet50-PoseNet</td>
            <td>ResNet50 (ImageNet)</td>
            <td>No</td>
            <td>Static-Œ≤ (Œ≤=0.01)</td>
            <td>50</td>
            <td>Adam</td>
            <td>1√ó10<sup>‚àí4</sup></td>
            <td>32</td>
          </tr>
          <tr>
            <td>ResNet50 + CBAM (ours)</td>
            <td>ResNet50 (ImageNet)</td>
            <td>Yes</td>
            <td>Learnable-Œ≤</td>
            <td>50</td>
            <td>Adam</td>
            <td>1√ó10<sup>‚àí4</sup></td>
            <td>32</td>
          </tr>
        </tbody>
      </table>
    </div>

    <h3>Data Preparation</h3>
    <ul>
      <li><strong>Image normalization:</strong> We scaled pixel values to [0, 1] and performed no further augmentation beyond random horizontal flipping (p=0.5).</li>
      <li><strong>Train/validation split:</strong> Followed the standard 70/30 split provided by the dataset maintainers.</li>
      <li><strong>Batching:</strong> Each mini-batch contained 32 images; each batch took ~0.5 s to process on an NVIDIA A100 GPU.</li>
    </ul>

    <h3>Freezing & Fine-Tuning</h3>
    <p>To stabilize early training, we froze the first two ResNet stages (conv1 & conv2_x) for epochs 1‚Äì10. After epoch 10, we unfroze all layers and continued fine-tuning at the same learning rate (1√ó10<sup>‚àí4</sup>).</p>

    <h3>Implementation Details</h3>
    <ul>
      <li>Framework: PyTorch 1.14, Python 3.10.</li>
      <li>Hardware: NVIDIA A100 GPU; each epoch took ‚âà 3 minutes for ResNet50 + CBAM on 9,950 training images.</li>
      <li>Checkpointing: Saved best validation model (lowest median translation error) every 5 epochs.</li>
    </ul>
  </section>

  <!-- Section 7: Results & Visualizations -->
  <section id="results">
    <h2>Loss Curves: What We Learned</h2>
  
    <p>We trained three architectures‚ÄîVGG16-PoseNet, ResNet50-PoseNet, and our enhanced ResNet50+CBAM model‚Äîunder two loss regimes: a learnable Œ≤ loss and a static Œ≤ (fixed-weight) loss. Below are the results, visualized under both <strong>linear</strong> and <strong>logarithmic</strong> scales.</p>
  
    <h3>(a) Learned Loss ‚Äî Linear Scale</h3>
    <img src="images/learned_loss_cbam.png" alt="Learned Œ≤ loss curve under linear scale">
    <p class="caption">CBAM (purple) not only converges fastest, but shows tight alignment between training and validation loss.</p>
  
    <h3>(b) Learned Loss ‚Äî Log Scale</h3>
    <img src="images/learned_loss_log.png" alt="Learned Œ≤ loss curve under log scale">
    <p class="caption">In log scale, ResNetCBAM decays nearly exponentially, while ResNet50 and VGG16 flatten out sooner or diverge slightly.</p>
  
    <h3>(c) Static Loss ‚Äî Linear Scale</h3>
    <img src="images/static_loss_cbam.png" alt="Static Œ≤ loss curve under linear scale">
    <p class="caption">Without a learned balance factor, all models struggle to reduce loss as effectively. Validation loss remains noticeably higher.</p>
  
    <h3>(d) Static Loss ‚Äî Log Scale</h3>
    <img src="images/static_loss_log.png" alt="Static Œ≤ loss curve under log scale">
    <p class="caption">Validation loss becomes noisy and plateaus earlier, especially for ResNet50 without CBAM (orange).</p>
  
    <h3>Takeaway</h3>
    <p>Across all views, ResNet50+CBAM achieves the <strong>fastest convergence</strong>, <strong>lowest final loss</strong>, and the <strong>tightest generalization gap</strong>. This suggests that CBAM helps the model focus on pose-relevant visual cues, while the <em>learned-Œ≤ loss</em> dynamically tunes the weight between translation and rotation errors‚Äîresulting in better stability and lower overfitting. We saw especially strong gains in the early training phase, where CBAM pushes the model into the ‚Äúlow-loss‚Äù zone earlier than any baseline.</p>
  </section>
  

  <!-- Section 8: Why CBAM Helps (Intuition + Evidence) -->
  <section id="why-cbam-helps">
    <h2>Why CBAM Helps (Intuition + Evidence)</h2>
    <h3>Intuition</h3>
    <ul>
      <li><strong>Channel Attention:</strong> At each ResNet stage, CBAM‚Äôs channel‚Äêattention module emphasizes features that are more informative for pose‚Äîe.g., edges of archways or distinct door frames‚Äîwhile deemphasizing redundant features (e.g., sky, ground, uniform textures).</li>
      <li><strong>Spatial Attention:</strong> CBAM‚Äôs spatial‚Äêattention module then localizes which areas of those key feature maps matter most. In King‚Äôs College, that might be a unique colored brick patch or a carved statue‚Äîsmall cues that anchor the pose.</li>
      <li><strong>End Result:</strong> Instead of processing every pixel equally, the network learns to ‚Äúzoom in‚Äù on the landmarks that actually determine position and orientation, making it more robust to distractions like pedestrians or changing sunlight.</li>
    </ul>

  

  <!-- Section 9: Discussion & Future Work -->
  <section id="discussion-future">
    <h2>Discussion & Future Work</h2>
    <h3>Limitations</h3>
    <ul>
      <li><strong>Scene Specificity:</strong> We only tested on the King‚Äôs College outdoor dataset. Indoor scenes (e.g., 7-Scenes) or multi‚Äêfloor environments (stairs, elevator lobbies) might show different performance trends.</li>
      <li><strong>Compute & Memory Overhead:</strong> CBAM adds ‚âà 2 GB of GPU memory usage (ResNet50 without CBAM: 7 GB ‚Üí ResNet50+CBAM: 9 GB on A100). On mobile or edge devices (Jetson Nano, Xavier), this could be a bottleneck.</li>
      <li><strong>Dynamic Obstacles:</strong> While our model handled occasional pedestrians, it was not explicitly trained for heavy crowd conditions. Adversarial distractors (moving bikes, flags) could still confuse the network.</li>
    </ul>

    <h3>Future Work</h3>
    <ul>
      <li><strong>Indoor Dataset Experiment:</strong> Next, we‚Äôll run on the <a href="https://www.microsoft.com/en-us/research/project/rgb-d-datase" target="_blank">7-Scenes dataset</a> to see how CBAM helps in low-light, repetitive indoor corridors.</li>
      <li><strong>Lighter Backbone:</strong> Explore ResNet18 + CBAM or MobileNetV2 + CBAM to enable real‚Äêtime inference on Jetson Nano/Xavier NX (target < 20 ms/frame).</li>
      <li><strong>Synthetic Data Augmentation:</strong> Use simulated environments (CARLA, AirSim) to generate diverse lighting/texture conditions and pre-train a hybrid model before fine-tuning on real data.</li>
      <li><strong>Temporal Consistency:</strong> Incorporate a lightweight LSTM or temporal smoothing on top of CBAM features to improve stability in video streams (reducing flicker in pose estimates).</li>
    </ul>
  </section>

  <!-- Section 10: Conclusion & Takeaways -->
  <section id="conclusion">
    <h2>Conclusion & Takeaways</h2>
    <p><strong>Recap:</strong> By replacing PoseNet‚Äôs VGG16 backbone with ResNet50 and injecting Convolutional Block Attention Modules, we achieved a 25 % reduction in translation error and a 35 % reduction in rotation error on the King‚Äôs College dataset. The learnable‚ÄêŒ≤ loss gave smoother convergence, while CBAM helped the network ‚Äúfocus‚Äù on salient landmark features (e.g., distinct brick patterns, archways) rather than distractions (e.g., sky, moving people).</p>
    <p><strong>Applications:</strong> Our ResNet50 + CBAM model can empower real‚Äêtime localization in safety‚Äêcritical robotics:</p>
    <ul>
      <li>Autonomous drones navigating forests or warehouses without GPS.</li>
      <li>AR/VR headsets overlaying graphics in changing lighting conditions.</li>
      <li>Smartphones providing indoor navigation in malls, museums, and urban canyons.</li>
    </ul>
    <p><strong>Code & Data:</strong> All code is publicly available on GitHub: <a href="https://github.com/yuertang/resnet-cbam-pose" target="_blank">github.com/yuertang/resnet-cbam-pose</a>. We used PyTorch 1.14 and the King‚Äôs College dataset (<a href="http://mi.eng.cam.ac.uk/projects/relocalisation/#king's-college" target="_blank">publicly available here</a>).</p>
    <p><strong>Invitation:</strong> Have questions? Want to test on your own scenes? Found a bug? Please open an issue or send me a note on GitHub. I‚Äôd love to collaborate!</p>
  </section>

  <!-- Section: References -->
  <section id="references">
    <h2>References & Resources</h2>
    <ul>
      <li>Kendall, A., Grimes, M., & Cipolla, R. (2015). <a href="https://arxiv.org/abs/1505.07427" target="_blank">PoseNet: A Convolutional Network for Real-Time 6-DOF Camera Relocalization</a>. *ICCV 2015 Workshop on Geometry Meets Deep Learning.*</li>
      <li>Woo, S., Park, J., Lee, J.-Y., & Kweon, I. (2018). <a href="https://arxiv.org/abs/1807.06521" target="_blank">CBAM: Convolutional Block Attention Module</a>. *ECCV 2018.*</li>
      <li>He, K., Zhang, X., Ren, S., & Sun, J. (2016). <a href="https://arxiv.org/abs/1512.03385" target="_blank">Deep Residual Learning for Image Recognition</a>. *CVPR 2016.*</li>
      <li>King‚Äôs College Dataset. (n.d.). <a href="http://mi.eng.cam.ac.uk/projects/relocalisation/#king's-college" target="_blank">http://mi.eng.cam.ac.uk/projects/relocalisation/#king's-college</a></li>
      <li>Nyhan, B., Chen, A. Y., Reifler, J., Robertson, R. E., & Wilson, C. (2023). <a href="https://doi.org/10.1126/sciadv.add8080" target="_blank">Subscriptions and external links help drive resentful users to alternative and extremist YouTube channels</a>. *Science Advances, 9*(35), eadd8080.</li>
    </ul>
  </section>

  <footer>
    <p>¬© 2025 Yuer Tang. All rights reserved.</p>
    <div class="sidebar-link">
      <a href="index.html">‚Üê Back to Home</a>
    </div>
  </footer>

</body>
</html>

